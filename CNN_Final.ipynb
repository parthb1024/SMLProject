{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import string\n",
    "from io import BytesIO\n",
    "from tensorflow.contrib import learn\n",
    "from collections import Counter\n",
    "from time import time\n",
    "import datetime\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"C:/Users/student/Downloads/consumer_complaints2.csv\", \n",
    "                usecols=('product','consumer_complaint_narrative'),\n",
    "                dtype={'consumer_complaint_narrative': object})\n",
    "# Only interested in data with consumer complaints\n",
    "d=d[d['consumer_complaint_narrative'].notnull()]\n",
    "d=d[d['product'].notnull()]\n",
    "d.reset_index(drop=True,inplace=True)\n",
    "u = d['product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data dimensions: (66806, 2)\n",
      "               product                       consumer_complaint_narrative\n",
      "0     Customer Service  XXXX has claimed I owe them {$27.00} for XXXX ...\n",
      "1     Customer Payment  Due to inconsistencies in the amount owed that...\n",
      "2  Internet Connection  In XX/XX/XXXX my wages that I earned at my job...\n",
      "3  Internet Connection  I have an open and current Internet Connection...\n",
      "4  Internet Connection  XXXX was submitted XX/XX/XXXX. At the time I s...\n",
      "\n",
      "List of Products       Occurrences\n",
      "\n",
      "Billing Issues                20455\n",
      "Customer Service              17552\n",
      "Internet Connection           14919\n",
      "Comcast account or service     5711\n",
      "Customer Payment               3678\n",
      "Slow Internet                  2128\n",
      "Prepaid card                    861\n",
      "Payday Internet                 726\n",
      "Money transfers                 666\n",
      "Other financial service         110\n",
      "Name: product, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print (\"Data dimensions:\", d.shape)\n",
    "print (d.head())\n",
    "\n",
    "# Let's see a table of how many examples we have of each product\n",
    "print (\"\\nList of Products       Occurrences\\n\")\n",
    "print (d[\"product\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning (partially modified)\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9()!?\\'\\`%$]\", \" \", string) # keep also %$ but removed comma\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" ( \", string)\n",
    "    string = re.sub(r\"\\)\", \" ) \", string)\n",
    "    string = re.sub(r\"\\?\", \" ? \", string)\n",
    "    string = re.sub(r\"\\$\", \" $ \", string) #yes, isolate $\n",
    "    string = re.sub(r\"\\%\", \" % \", string) #yes, isolate %\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    # fixing XXX and xxx like as word\n",
    "    string = re.sub(r'\\S*(x{2,}|X{2,})\\S*',\"xxx\",string)\n",
    "    # removing non ascii\n",
    "    string = re.sub(r'[^\\x00-\\x7F]+', \"\", string) \n",
    "    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_data=[]\n",
    "t0 = time()\n",
    "\n",
    "for message in d['consumer_complaint_narrative']:\n",
    "    word_data.append(clean_str(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: the reference product is Customer Service\n",
      "\n",
      "** Before cleaning ** \n",
      "\n",
      "After retaining counsel in XXXX of XXXX due to contact by Javitch & Co. a collection agency in XXXX OH for judgement on a Billing Issues debt I could no longer pay on due to having lost my job in XXXX of XXXX ... said collection agency sent me on consecutive months from XXXX till XXXX, requests fro payments ... Attorney XXXX XXXX XXXX OH contacted said collection firm and asked to set up a payment schedule - debited directly from my Comcast account. Attorney XXXX NEVER had a call returned from XXXX XXXX the contact person on record for this action with Javitch & Co. in XXXX OH. I am on Social Security, as my only source of income. \n",
      "Being a seasonal employee at XXXX XXXX in XXXX - I was contacted by XXXX XXXX that my wages would be garnished at 25 % of net pay, when I worked for XXXX XXXX - working for XXXX XXXX from XXXX XXXX to XXXX XXXX, XXXX - I had {$2800.00} garnished from my paychecks - a MAJOR setback for my finances as my Social Security barely covers my monthly overhead. \n",
      "Efforts by Attorney XXXX to reach XXXX XXXX at Javitch & Co. to arrive at an equitable monthly payment - directly debited from my account. Went unanswered by phone and writing. \n",
      "A complete written history since judgement was rendered by the court toward my effort to establish payment schedule, through counsel is 100 % factual. This Collection agency - Javitch & Co. has totally ignored all requests by counsel & has circumvented equitable protocol to establish a payment schedule for re-payment of this debt. \n",
      "\n",
      "** After cleaning ** \n",
      "\n",
      "after retaining counsel in xxx of xxx due to contact by javitch co a collection agency in xxx oh for judgement on a billing issues debt i could no longer pay on due to having lost my job in xxx of xxx said collection agency sent me on consecutive months from xxx till xxx requests fro payments attorney xxx xxx xxx oh contacted said collection firm and asked to set up a payment schedule debited directly from my comcast account attorney xxx never had a call returned from xxx xxx the contact person on record for this action with javitch co in xxx oh i am on social security as my only source of income being a seasonal employee at xxx xxx in xxx i was contacted by xxx xxx that my wages would be garnished at 25 % of net pay when i worked for xxx xxx working for xxx xxx from xxx xxx to xxx xxx xxx i had $ 2800 00 garnished from my paychecks a major setback for my finances as my social security barely covers my monthly overhead efforts by attorney xxx to reach xxx xxx at javitch co to arrive at an equitable monthly payment directly debited from my account went unanswered by phone and writing a complete written history since judgement was rendered by the court toward my effort to establish payment schedule through counsel is 100 % factual this collection agency javitch co has totally ignored all requests by counsel has circumvented equitable protocol to establish a payment schedule for re payment of this debt\n"
     ]
    }
   ],
   "source": [
    "an_example = 38\n",
    "print (\"Note: the reference product is\",d ['product'][an_example])\n",
    "print (\"\\n** Before cleaning ** \\n\")\n",
    "print (d['consumer_complaint_narrative'][an_example])\n",
    "print (\"** After cleaning ** \\n\")\n",
    "print (word_data [an_example])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_document_length: 910\n",
      "Vocabulary Size: 52925\n"
     ]
    }
   ],
   "source": [
    "max_document_length = max([len(x.split(\" \")) for x in word_data])\n",
    "print (\"Max_document_length:\",max_document_length)\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "num_data = np.array(list(vocab_processor.fit_transform(word_data)))\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check my variables:\n",
      "\n",
      "* word_data length: 66806\n",
      "* num_data length:  66806\n",
      "\n",
      "Products:\n",
      "* data length:  10\n",
      "* labels:\n",
      " ['Internet Connection', 'Payday Internet', 'Money transfers', 'Slow Internet', 'Customer Payment', 'Prepaid card', 'Customer Service', 'Billing Issues', 'Other financial service', 'Comcast account or service']\n"
     ]
    }
   ],
   "source": [
    "print (\"Check my variables:\")\n",
    "print (\"\\n* word_data length:\", len(word_data))\n",
    "print (\"* num_data length: \", len(num_data)) # once words are numbers\n",
    "\n",
    "#Create the list of products\n",
    "product_labels = list(set(d['product']))\n",
    "print (\"\\nProducts:\")\n",
    "print (\"* data length: \",len(product_labels))\n",
    "print (\"* labels:\\n\",product_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* x shuffled: (66806, 910)\n",
      "* y shuffled: (66806,)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(57)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(num_data)))\n",
    "x_shuffled = num_data[shuffle_indices]\n",
    "y_shuffled = d['product'][shuffle_indices]\n",
    "print (\"* x shuffled:\", x_shuffled.shape)\n",
    "print (\"* y shuffled:\", y_shuffled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set   (40083, 910) (40083,)\n",
      "Validation set (13361, 910) (13361,)\n",
      "Test set       (13362, 910) (13362,)\n"
     ]
    }
   ],
   "source": [
    "features_dummy, x_test, labels_dummy, test_labels = model_selection.train_test_split(x_shuffled, y_shuffled, test_size=0.20, random_state= 23)\n",
    "x_train, x_valid, train_labels, valid_labels = model_selection.train_test_split(features_dummy, labels_dummy, test_size=0.25, random_state= 34)\n",
    "\n",
    "print('Training set  ',   x_train.shape, train_labels.shape)\n",
    "print('Validation set',   x_valid.shape, valid_labels.shape)\n",
    "print('Test set      ',    x_test.shape,  test_labels.shape)\n",
    "\n",
    "# free some memory\n",
    "del num_data, d \n",
    "del x_shuffled, y_shuffled, labels_dummy, features_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-128\"):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [int(filter_size), embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, int(sequence_length) - int(filter_size) + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1], \n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        print(pooled_outputs)\n",
    "        self.h_pool = tf.concat(pooled_outputs,3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\") \n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            print (self.scores)\n",
    "            print (self.input_y)\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " * OneHot example\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " * OneHot example\n",
      "[[ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " * OneHot example\n",
      "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  1. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "def oneHot(dummy_labels):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    enc = OneHotEncoder()\n",
    "    \n",
    "    le.fit (dummy_labels)\n",
    "    y_dummy = le.fit_transform(dummy_labels)\n",
    "    y_dummy = y_dummy.reshape(-1, 1)\n",
    "    enc.fit(y_dummy)\n",
    "    y_dummy = enc.transform(y_dummy).toarray()\n",
    "    y_dummy = y_dummy.astype('float32')\n",
    "    print (\"\\n * OneHot example\")\n",
    "    print (y_dummy)\n",
    "    return (y_dummy)\n",
    "        \n",
    "y_train = oneHot(train_labels)\n",
    "y_valid = oneHot(valid_labels)\n",
    "y_test  = oneHot( test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'conv-maxpool-128/pool:0' shape=(?, 1, 1, 128) dtype=float32>, <tf.Tensor 'conv-maxpool-128_1/pool:0' shape=(?, 1, 1, 128) dtype=float32>, <tf.Tensor 'conv-maxpool-128_2/pool:0' shape=(?, 1, 1, 128) dtype=float32>]\n",
      "Tensor(\"output/scores:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"input_y:0\", shape=(?, 10), dtype=float32)\n",
      "WARNING:tensorflow:From <ipython-input-12-b327e969dc9d>:77: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-128/W:0/grad/hist is illegal; using conv-maxpool-128/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-128/W:0/grad/sparsity is illegal; using conv-maxpool-128/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-128/b:0/grad/hist is illegal; using conv-maxpool-128/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-128/b:0/grad/sparsity is illegal; using conv-maxpool-128/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-128_1/W:0/grad/hist is illegal; using conv-maxpool-128_1/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-128_1/W:0/grad/sparsity is illegal; using conv-maxpool-128_1/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-128_1/b:0/grad/hist is illegal; using conv-maxpool-128_1/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-128_1/b:0/grad/sparsity is illegal; using conv-maxpool-128_1/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-128_2/W:0/grad/hist is illegal; using conv-maxpool-128_2/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-128_2/W:0/grad/sparsity is illegal; using conv-maxpool-128_2/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-128_2/b:0/grad/hist is illegal; using conv-maxpool-128_2/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-128_2/b:0/grad/sparsity is illegal; using conv-maxpool-128_2/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "WARNING:tensorflow:From E:\\Programs\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Validation set: 2018-04-06T19:14:04.161400, step 100, loss 2.8889, acc 0.391128\n",
      "Validation set: 2018-04-06T19:19:56.564711, step 200, loss 2.11381, acc 0.512212\n",
      "Validation set: 2018-04-06T19:25:58.429703, step 300, loss 1.80241, acc 0.554296\n",
      "Validation set: 2018-04-06T19:31:43.792437, step 400, loss 1.41659, acc 0.616692\n",
      "Validation set: 2018-04-06T19:37:28.200205, step 500, loss 1.23701, acc 0.635004\n",
      "Validation set: 2018-04-06T19:43:21.130512, step 600, loss 1.1063, acc 0.671613\n",
      "\n",
      "\n",
      "Test set:\n",
      "2018-04-06T19:46:22.752180: step 627, loss 1.10235, acc 0.678654\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=True,\n",
    "      log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=len(product_labels),\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=128,\n",
    "            filter_sizes=\"234\",\n",
    "            num_filters = 128,\n",
    "            l2_reg_lambda= 0.0)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        #Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries (if needed)\n",
    "        \n",
    "        #timestamp = str(int(time()))\n",
    "        #out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        #print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        #train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        #train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        #dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        #dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory (if needed)\n",
    "        # Tensorflow assumes this directory already exists so we need to create it\n",
    "        \n",
    "        #checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        #checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        #if not os.path.exists(checkpoint_dir):\n",
    "        #    os.makedirs(checkpoint_dir)\n",
    "        #saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "        # Write vocabulary (if needed)\n",
    "        #vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 0.5\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            \n",
    "            # Uncomment next print if interested in batch results \n",
    "            #print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            \n",
    "            #train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 0.5\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            return loss, accuracy, summaries\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), 64, 1)\n",
    "        \n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            \n",
    "# Validating\n",
    "# ==================================================\n",
    "            if current_step % 100 == 0:\n",
    "                #print(\"\\nEvaluation:\")\n",
    "                \n",
    "                # Generate batches\n",
    "                batches_valid = batch_iter(\n",
    "                    list(zip(x_valid, y_valid)), 64, 1)\n",
    "                \n",
    "                loss_valid = 0.\n",
    "                acc_valid = 0.\n",
    "                len_batches = 0.\n",
    "                \n",
    "                for batch_valid in batches_valid:  \n",
    "                    \n",
    "                    x_batch_valid, y_batch_valid = zip(*batch_valid)\n",
    "                    #aLoss, anAcc, aSummary = dev_step(x_batch_valid, y_batch_valid, writer=dev_summary_writer)\n",
    "                    aLoss, anAcc, aSummary = dev_step(x_batch_valid, y_batch_valid)\n",
    "                    loss_valid += aLoss \n",
    "                    acc_valid  += anAcc\n",
    "                    len_batches += 1.\n",
    "                \n",
    "                loss_valid = loss_valid / len_batches\n",
    "                acc_valid  = acc_valid  / len_batches \n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"Validation set: {}, step {}, loss {:g}, acc {:g}\".format(time_str, current_step, loss_valid, acc_valid))\n",
    "                #dev_summary_writer.add_summary(aSummary, current_step)\n",
    "                #print(\"\")\n",
    "                \n",
    "            #if current_step % FLAGS.checkpoint_every == 0:\n",
    "            #    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "            #    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "    \n",
    "        \n",
    "# Testing\n",
    "# ==================================================\n",
    "        if True:\n",
    "            print(\"\\n\\nTest set:\")\n",
    "            \n",
    "            # Generate batches\n",
    "            batches_test = batch_iter(\n",
    "                list(zip(x_test, y_test)), 64, 1)\n",
    "        \n",
    "            loss_test = 0.\n",
    "            acc_test  = 0.\n",
    "            len_batches = 0.\n",
    "            \n",
    "            for batch_test in batches_test:  \n",
    "                    \n",
    "                    x_batch_test, y_batch_test = zip(*batch_test)\n",
    "                    #aLoss, anAcc, aSummary = dev_step(x_batch_test, y_batch_test, writer=dev_summary_writer)\n",
    "                    aLoss, anAcc, aSummary = dev_step(x_batch_test, y_batch_test)\n",
    "                    loss_test += aLoss \n",
    "                    acc_test  += anAcc\n",
    "                    len_batches += 1.\n",
    "                \n",
    "            loss_test = loss_test / len_batches\n",
    "            acc_test  = acc_test  / len_batches \n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, current_step, loss_test, acc_test))\n",
    "            #dev_summary_writer.add_summary(aSummary, current_step)\n",
    "            print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
